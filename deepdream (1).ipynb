{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SgrstLXNbG_"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:35.722517Z",
     "iopub.status.busy": "2020-10-07T01:27:35.721803Z",
     "iopub.status.idle": "2020-10-07T01:27:35.724111Z",
     "shell.execute_reply": "2020-10-07T01:27:35.724527Z"
    },
    "id": "k7gifg92NbG9"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from Functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCMqzy7BNbG9"
   },
   "source": [
    "# DeepDream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yqCPS8SNbG8"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/deepdream\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/deepdream.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPDKhwPcNbG7"
   },
   "source": [
    "This tutorial contains a minimal implementation of DeepDream, as described in this [blog post](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) by Alexander Mordvintsev.\n",
    "\n",
    "DeepDream is an experiment that visualizes the patterns learned by a neural network. Similar to when a child watches clouds and tries to interpret random shapes, DeepDream over-interprets and enhances the patterns it sees in an image.\n",
    "\n",
    "It does so by forwarding an image through the network, then calculating the gradient of the image with respect to the activations of a particular layer. The image is then modified to increase these activations, enhancing the patterns seen by the network, and resulting in a dream-like image. This process was dubbed \"Inceptionism\" (a reference to [InceptionNet](https://arxiv.org/pdf/1409.4842.pdf), and the [movie](https://en.wikipedia.org/wiki/Inception) Inception).\n",
    "\n",
    "Let's demonstrate how you can make a neural network \"dream\" and enhance the surreal patterns it sees in an image.\n",
    "\n",
    "![Dogception](https://www.tensorflow.org/tutorials/generative/images/dogception.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:35.728573Z",
     "iopub.status.busy": "2020-10-07T01:27:35.727892Z",
     "iopub.status.idle": "2020-10-07T01:27:41.967798Z",
     "shell.execute_reply": "2020-10-07T01:27:41.966864Z"
    },
    "id": "Sc5Yq_Rgxreb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:41.972526Z",
     "iopub.status.busy": "2020-10-07T01:27:41.971824Z",
     "iopub.status.idle": "2020-10-07T01:27:42.001484Z",
     "shell.execute_reply": "2020-10-07T01:27:42.000974Z"
    },
    "id": "g_Qp173_NbG5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import IPython.display as display\n",
    "import PIL.Image\n",
    "\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgeIJg82NbG4"
   },
   "source": [
    "## Choose an image to dream-ify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt6zam_9NbG4"
   },
   "source": [
    "For this tutorial, let's use an image of a [labrador](https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:42.005569Z",
     "iopub.status.busy": "2020-10-07T01:27:42.004929Z",
     "iopub.status.idle": "2020-10-07T01:27:42.007126Z",
     "shell.execute_reply": "2020-10-07T01:27:42.006529Z"
    },
    "id": "0lclzk9sNbG2"
   },
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:42.014969Z",
     "iopub.status.busy": "2020-10-07T01:27:42.014276Z",
     "iopub.status.idle": "2020-10-07T01:27:42.278703Z",
     "shell.execute_reply": "2020-10-07T01:27:42.278141Z"
    },
    "id": "Y5BPgc8NNbG0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x7FBC1CF21730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48)\n",
      "(250, 250)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAAAAAByaaZbAAAEyElEQVR4nJXOQZYlyW0EQHcHIvNXN4ekRpfhMbXSBbWZlch+ZFX9jADgXOgEshMY/xv/L//Fvz3/W2d326nDK1foiopYwLn0zunDPjukYw/+M/3+er3qiNeDsK7BZd1ag1DzAugOihzf30Lk+8Ejsj0YpxjwJXESkMbRGAMEzLq6kbW3fJwl9wo5RMKRnMnRbOySG7Bj3Al9PmulUBlYKfFaF+JeMyYXHA40LY6DZrUaEdhYCDLZLGbEOEKZiIvUdRFl2iMKWVNsTLOxyFh5L5opjCya8RbP8qMcl8z8xDUTmJh1hbQulzLCsbQHBu86YAs1GisUq4GNJalpBW5FCIuRNyLupdfHJV4wbU6nnJ+brZjRDp/VSRqUOlDcjmYAk/b3GjBRX7222q+ZQFSf1fuOCapm7zZzYvQIq6YD+ZjJvJ8rEtc1sm3UXMSUvCbqdDS1ydWg0/TYT8CZKaZJO0Y9OCNHXPIUz7hgFZPclpQMnCsyvIf9XmfpYPrcDmZfkPaDnOqcuBv6OBpdWYwQSkGE3Xb2rIVr1zgSc8TsJhmTicL+8zp0l1/WUfVu7vsrPmp3b7zmXN8rrz4OrurmfT7/4x9//cbWzmizAOcZNJ7sk9vRsirWS9pwfIz6f5x/2n0YDHydAT9+X3t9+/urcWQLGQDW3EoGgbv/aEwYyR/1tIeI/LX/UuPyCWOy13r/bC6tXhcgz1tY2fGtZXmevfaPudMYylQm6xIlxuowMRWD90VCMfuD87r+FO+aoKvicRqautdyLF8DDmr0U/fbVPAyRvg0bxOx58ps3fsy+eMSSG8AqUEF2X1xcWoHySFOXD4JT2AtZTCNvr5Nad1kbyZYWn6dmMU9EbuvVEUImWshVDz1RL8+LGiswGUYN6H5+X0OMxI+ciCUkyxz0xIUG+E2P1ahlYPEshpKK0DovrjQud9xD9bKjD1i1ddvloZxuZRfM04abHFaw6490rz08dKzwxOrHb0X7kDG5/We0QzhPWbPfn+XxOWws1/De0lDsOdMn+2BJ6m6lGRlfwW6T/Hu7ec00+70EO4c1wwCSnWMnlern5ozT8iojTHT7ciRr3o4V9tudmrqxupSPzzvJp7YgMY723c5Ck59R6m76vzMzO5glzv3tztKGeyK7+U5c7s9qub2sCE4747ujmTt0fDcN+NWzA/mju7qGCtObFhvmfm+uKJQNx8fRCyTt3RBQLJawGzxeeHsdcJKcqqnEUuSBlhXFwvhDI7ZUwx7KDdPpjorZ+Z9Na6D9fpZ18olN8nMnvYvixKpQUzuNdMG35R94fjH+g2/P/ptft2fv/+hd/89S45VpgYn82RMbbAHrLj352/1One+eAb/mOdf//zWYs7WwN3OJ7vEee4hEEWeX81s7dm1v/aukACP4W5Updv24OZzW4ff92T9+sxge7b79JuCHKdMdFts16lGc1OhIAD3zJhYGFhGbgDTPJ/q9I4KVpvEpMFpE7G2J/R0GURdaBzVic+f+bhYOc8iLs3iHBXjsY9P7e0e0ZzpwaAzskGwcZDxfrlS0dvZz4x4pmNCnqoBz6nx/yVyNBXGRif2iw3kJsHMWu2+9qA8taeRYKtt5fQ1Nwg6Ylgra1THmMx9Zq96l+36N99CAEj9eBSWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x7FBC1CEEAE20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download an image and read it into a NumPy array.\n",
    "def download(url):\n",
    "  name = url.split('/')[-1]\n",
    "  image_path = tf.keras.utils.get_file(name, origin=url)\n",
    "  img = PIL.Image.open(image_path)\n",
    "  \n",
    "  return np.array(img)\n",
    "\n",
    "# Normalize an image\n",
    "def deprocess(img):\n",
    "  img = 255*img\n",
    "  \n",
    "  return tf.cast(img, tf.uint8)\n",
    "\n",
    "# Display an image\n",
    "def show(img):\n",
    "  display.display(PIL.Image.fromarray(np.array(img)))\n",
    "\n",
    "\n",
    "# Downsizing the image makes it easier to work with.\n",
    "original_img = download(url)\n",
    "original_img = cv2.resize(original_img, (48,48))\n",
    "original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)\n",
    "show(original_img)\n",
    "display.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))\n",
    "print(original_img.shape)\n",
    "face=pkl.load(open('myface.pkl','rb'))\n",
    "print(face[0].shape)\n",
    "face = cv2.resize(face[1], (48,48))\n",
    "\n",
    "show(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4RBFfIWNbG0"
   },
   "source": [
    "## Prepare the feature extraction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cruNQmMDNbGz"
   },
   "source": [
    "Download and prepare a pre-trained image classification model. You will use [InceptionV3](https://keras.io/applications/#inceptionv3) which is similar to the model originally used in DeepDream. Note that any [pre-trained model](https://keras.io/applications/#models-for-image-classification-with-weights-trained-on-imagenet) will work, although you will have to adjust the layer names below if you change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:42.285544Z",
     "iopub.status.busy": "2020-10-07T01:27:42.284913Z",
     "iopub.status.idle": "2020-10-07T01:27:55.668527Z",
     "shell.execute_reply": "2020-10-07T01:27:55.667890Z"
    },
    "id": "GlLi48GKNbGy"
   },
   "outputs": [],
   "source": [
    "model=my_model() # create the model\n",
    "model.load_weights('FinalModel')\n",
    "base_model =  model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bujb0jPNNbGx"
   },
   "source": [
    "The idea in DeepDream is to choose a layer (or layers) and maximize the \"loss\" in a way that the image increasingly \"excites\" the layers. The complexity of the features incorporated depends on layers chosen by you, i.e, lower layers produce strokes or simple patterns, while deeper layers give sophisticated features in images, or even whole objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOVmDO4LNbGv"
   },
   "source": [
    "The InceptionV3 architecture is quite large (for a graph of the model architecture see TensorFlow's [research repo](https://github.com/tensorflow/models/tree/master/research/inception)). For DeepDream, the layers of  interest are those where the convolutions are concatenated. There are 11 of these layers in InceptionV3, named 'mixed0' though 'mixed10'. Using different layers will result in different dream-like images. Deeper layers respond to higher-level features (such as eyes and faces), while earlier layers respond to simpler features (such as edges, shapes, and textures). Feel free to experiment with the layers selected below, but keep in mind that deeper layers (those with a higher index) will take longer to train on since the gradient computation is deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:55.675998Z",
     "iopub.status.busy": "2020-10-07T01:27:55.675320Z",
     "iopub.status.idle": "2020-10-07T01:27:55.692441Z",
     "shell.execute_reply": "2020-10-07T01:27:55.691863Z"
    },
    "id": "08KB502ONbGt"
   },
   "outputs": [],
   "source": [
    "# Maximize the activations of these layers\n",
    "names = [name.name for name in  base_model.layers[:9] ]\n",
    "layers = [base_model.get_layer(name).output for name in names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb7u31B4NbGt"
   },
   "source": [
    "## Calculate loss\n",
    "\n",
    "The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. Normally, loss is a quantity you wish to minimize via gradient descent. In DeepDream, you will maximize this loss via gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:55.697798Z",
     "iopub.status.busy": "2020-10-07T01:27:55.697069Z",
     "iopub.status.idle": "2020-10-07T01:27:55.699181Z",
     "shell.execute_reply": "2020-10-07T01:27:55.699541Z"
    },
    "id": "8MhfSweXXiuq"
   },
   "outputs": [],
   "source": [
    "def calc_loss(img, model):\n",
    "  # Pass forward the image through the model to retrieve the activations.\n",
    "  # Converts the image into a batch of size 1.\n",
    "  img_batch = tf.expand_dims(img, axis=0)\n",
    "  layer_activations = model(img_batch)\n",
    "  if len(layer_activations) == 1:\n",
    "    layer_activations = [layer_activations]\n",
    "\n",
    "  losses = []\n",
    "  for act in layer_activations:\n",
    "    loss = tf.math.reduce_mean(act)\n",
    "    losses.append(loss)\n",
    "\n",
    "  return  tf.reduce_sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4TCNsAUO9kI"
   },
   "source": [
    "## Gradient ascent\n",
    "\n",
    "Once you have calculated the loss for the chosen layers, all that is left is to calculate the gradients with respect to the image, and add them to the original image. \n",
    "\n",
    "Adding the gradients to the image enhances the patterns seen by the network. At each step, you will have created an image that increasingly excites the activations of certain layers in the network.\n",
    "\n",
    "The method that does this, below, is wrapped in a `tf.function` for performance. It uses an `input_signature` to ensure that the function is not retraced for different image sizes or `steps`/`step_size` values. See the [Concrete functions guide](../../guide/concrete_function.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:55.707388Z",
     "iopub.status.busy": "2020-10-07T01:27:55.706680Z",
     "iopub.status.idle": "2020-10-07T01:27:55.708603Z",
     "shell.execute_reply": "2020-10-07T01:27:55.708954Z"
    },
    "id": "qRScWg_VNqvj"
   },
   "outputs": [],
   "source": [
    "class DeepDream(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "\n",
    "  def __call__(self, img, steps, step_size):\n",
    "      print(\"Tracing\")\n",
    "      loss = tf.constant(0.0)\n",
    "      for n in tf.range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "          # This needs gradients relative to `img`\n",
    "          # `GradientTape` only watches `tf.Variable`s by default\n",
    "          tape.watch(img)\n",
    "          loss = calc_loss(img, self.model)\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
    "        gradients = tape.gradient(loss, img)\n",
    "\n",
    "        # Normalize the gradients.\n",
    "        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "        \n",
    "        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
    "        # You can update the image by directly adding the gradients (because they're the same shape!)\n",
    "        img = img + gradients*step_size\n",
    "        img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "      return loss, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:55.712735Z",
     "iopub.status.busy": "2020-10-07T01:27:55.712046Z",
     "iopub.status.idle": "2020-10-07T01:27:55.714191Z",
     "shell.execute_reply": "2020-10-07T01:27:55.714649Z"
    },
    "id": "yB9pTqn6xfuK"
   },
   "outputs": [],
   "source": [
    "deepdream = DeepDream(dream_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLArRTVHZFAi"
   },
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:55.721124Z",
     "iopub.status.busy": "2020-10-07T01:27:55.720439Z",
     "iopub.status.idle": "2020-10-07T01:27:55.722380Z",
     "shell.execute_reply": "2020-10-07T01:27:55.722808Z"
    },
    "id": "9vHEcy7dTysi"
   },
   "outputs": [],
   "source": [
    "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n",
    "  # Convert from uint8 to the range expected by the model.\n",
    "  img = tf.convert_to_tensor(img)\n",
    "  step_size = tf.convert_to_tensor(step_size)\n",
    "  steps_remaining = steps\n",
    "  step = 0\n",
    "  while steps_remaining:\n",
    "    if steps_remaining>100:\n",
    "      run_steps = tf.constant(100)\n",
    "    else:\n",
    "      run_steps = tf.constant(steps_remaining)\n",
    "    steps_remaining -= run_steps\n",
    "    step += run_steps\n",
    "\n",
    "    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    show(deprocess(img))\n",
    "    print (\"Step {}, loss {}\".format(step, loss))\n",
    "\n",
    "\n",
    "  result = deprocess(img)\n",
    "  display.clear_output(wait=True)\n",
    "  show(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:27:55.726216Z",
     "iopub.status.busy": "2020-10-07T01:27:55.725626Z",
     "iopub.status.idle": "2020-10-07T01:28:00.577217Z",
     "shell.execute_reply": "2020-10-07T01:28:00.577684Z"
    },
    "id": "tEfd00rr0j8Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.uint8\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.uint8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-858d789e607e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dream_img = run_deep_dream_simple(img=original_img,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   steps=1000, step_size=0.000001)\n",
      "\u001b[0;32m<ipython-input-11-ca2342081286>\u001b[0m in \u001b[0;36mrun_deep_dream_simple\u001b[0;34m(img, steps, step_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepdream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6c1999f423c6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img, steps, step_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Normalize the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_std\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2419\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"reduce_std\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2421\u001b[0;31m     \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_variance\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2358\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"reduce_variance\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input must be either real or complex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2308\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m       gen_math_ops.mean(\n\u001b[0;32m-> 2310\u001b[0;31m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m           name=name))\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis, reduction_indices)\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mrank\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m   \"\"\"\n\u001b[0;32m--> 816\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mrank_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mrank_internal\u001b[0;34m(input, name, optimize)\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                          as_ref=False):\n\u001b[1;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[0;32m--> 263\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "dream_img = run_deep_dream_simple(img=original_img,\n",
    "                                  steps=1000, step_size=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = face.reshape(48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(cv2.resize(dream_img.numpy(), (500,500)),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PbfXEVFNbGp"
   },
   "source": [
    "## Taking it up an octave\n",
    "\n",
    "Pretty good, but there are a few issues with this first attempt: \n",
    "\n",
    "  1. The output is noisy (this could be addressed with a `tf.image.total_variation` loss).\n",
    "  1. The image is low resolution.\n",
    "  1. The patterns appear like they're all happening at the same granularity.\n",
    "  \n",
    "One approach that addresses all these problems is applying gradient ascent at different scales. This will allow patterns generated at smaller scales to be incorporated into patterns at higher scales and filled in with additional detail.\n",
    "\n",
    "To do this you can perform the previous gradient ascent approach, then increase the size of the image (which is referred to as an octave), and repeat this process for multiple octaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:00.585229Z",
     "iopub.status.busy": "2020-10-07T01:28:00.584608Z",
     "iopub.status.idle": "2020-10-07T01:28:06.131542Z",
     "shell.execute_reply": "2020-10-07T01:28:06.131045Z"
    },
    "id": "0eGDSdatLT-8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "OCTAVE_SCALE = 1.30\n",
    "\n",
    "img = tf.constant(np.array(original_img))\n",
    "base_shape = tf.shape(img)[:-1]\n",
    "float_base_shape = tf.cast(base_shape, tf.float32)\n",
    "\n",
    "for n in range(-2, 3):\n",
    "  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape).numpy()\n",
    "\n",
    "  img = run_deep_dream_simple(img=img, steps=50, step_size=0.01)\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "img = tf.image.resize(img, base_shape)\n",
    "img = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\n",
    "show(img)\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9xqyeuwLZFy"
   },
   "source": [
    "## Optional: Scaling up with tiles\n",
    "\n",
    "One thing to consider is that as the image increases in size, so will the time and memory necessary to perform the gradient calculation. The above octave implementation will not work on very large images, or many octaves.\n",
    "\n",
    "To avoid this issue you can split the image into tiles and compute the gradient for each tile.\n",
    "\n",
    "Applying random shifts to the image before each tiled computation prevents tile seams from appearing.\n",
    "\n",
    "Start by implementing the random shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:06.136871Z",
     "iopub.status.busy": "2020-10-07T01:28:06.136186Z",
     "iopub.status.idle": "2020-10-07T01:28:06.138214Z",
     "shell.execute_reply": "2020-10-07T01:28:06.138620Z"
    },
    "id": "oGgLHk7o80ac"
   },
   "outputs": [],
   "source": [
    "def random_roll(img, maxroll):\n",
    "  # Randomly shift the image to avoid tiled boundaries.\n",
    "  shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n",
    "  img_rolled = tf.roll(img, shift=shift, axis=[0,1])\n",
    "  return shift, img_rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:06.142435Z",
     "iopub.status.busy": "2020-10-07T01:28:06.141775Z",
     "iopub.status.idle": "2020-10-07T01:28:06.224106Z",
     "shell.execute_reply": "2020-10-07T01:28:06.224543Z"
    },
    "id": "sKsiqWfA9H41"
   },
   "outputs": [],
   "source": [
    "shift, img_rolled = random_roll(np.array(original_img), 512)\n",
    "show(img_rolled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGIjA3UhhAt8"
   },
   "source": [
    "Here is a tiled equivalent of the `deepdream` function defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:06.235016Z",
     "iopub.status.busy": "2020-10-07T01:28:06.234137Z",
     "iopub.status.idle": "2020-10-07T01:28:06.236472Z",
     "shell.execute_reply": "2020-10-07T01:28:06.235935Z"
    },
    "id": "x__TZ0uqNbGm"
   },
   "outputs": [],
   "source": [
    "class TiledGradients(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(\n",
    "      input_signature=(\n",
    "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int32),)\n",
    "  )\n",
    "  def __call__(self, img, tile_size=512):\n",
    "    shift, img_rolled = random_roll(img, tile_size)\n",
    "\n",
    "    # Initialize the image gradients to zero.\n",
    "    gradients = tf.zeros_like(img_rolled)\n",
    "    \n",
    "    # Skip the last tile, unless there's only one tile.\n",
    "    xs = tf.range(0, img_rolled.shape[0], tile_size)[:-1]\n",
    "    if not tf.cast(len(xs), bool):\n",
    "      xs = tf.constant([0])\n",
    "    ys = tf.range(0, img_rolled.shape[1], tile_size)[:-1]\n",
    "    if not tf.cast(len(ys), bool):\n",
    "      ys = tf.constant([0])\n",
    "\n",
    "    for x in xs:\n",
    "      for y in ys:\n",
    "        # Calculate the gradients for this tile.\n",
    "        with tf.GradientTape() as tape:\n",
    "          # This needs gradients relative to `img_rolled`.\n",
    "          # `GradientTape` only watches `tf.Variable`s by default.\n",
    "          tape.watch(img_rolled)\n",
    "\n",
    "          # Extract a tile out of the image.\n",
    "          img_tile = img_rolled[x:x+tile_size, y:y+tile_size]\n",
    "          loss = calc_loss(img_tile, self.model)\n",
    "\n",
    "        # Update the image gradients for this tile.\n",
    "        gradients = gradients + tape.gradient(loss, img_rolled)\n",
    "\n",
    "    # Undo the random shift applied to the image and its gradients.\n",
    "    gradients = tf.roll(gradients, shift=-shift, axis=[0,1])\n",
    "\n",
    "    # Normalize the gradients.\n",
    "    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "\n",
    "    return gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:06.239991Z",
     "iopub.status.busy": "2020-10-07T01:28:06.239375Z",
     "iopub.status.idle": "2020-10-07T01:28:06.241265Z",
     "shell.execute_reply": "2020-10-07T01:28:06.241671Z"
    },
    "id": "Vcq4GubA2e5J"
   },
   "outputs": [],
   "source": [
    "get_tiled_gradients = TiledGradients(dream_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYnTTs_qiaND"
   },
   "source": [
    "Putting this together gives a scalable, octave-aware deepdream implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:06.249322Z",
     "iopub.status.busy": "2020-10-07T01:28:06.248586Z",
     "iopub.status.idle": "2020-10-07T01:28:06.250738Z",
     "shell.execute_reply": "2020-10-07T01:28:06.250256Z"
    },
    "id": "gA-15DM4NbGk"
   },
   "outputs": [],
   "source": [
    "def run_deep_dream_with_octaves(img, steps_per_octave=100, step_size=0.01, \n",
    "                                octaves=range(-2,3), octave_scale=1.3):\n",
    "  base_shape = tf.shape(img)\n",
    "  img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "  initial_shape = img.shape[:-1]\n",
    "  img = tf.image.resize(img, initial_shape)\n",
    "  for octave in octaves:\n",
    "    # Scale the image based on the octave\n",
    "    new_size = tf.cast(tf.convert_to_tensor(base_shape[:-1]), tf.float32)*(octave_scale**octave)\n",
    "    img = tf.image.resize(img, tf.cast(new_size, tf.int32))\n",
    "\n",
    "    for step in range(steps_per_octave):\n",
    "      gradients = get_tiled_gradients(img)\n",
    "      img = img + gradients*step_size\n",
    "      img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "      if step % 10 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "        show(deprocess(img))\n",
    "        print (\"Octave {}, Step {}\".format(octave, step))\n",
    "    \n",
    "  result = deprocess(img)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-07T01:28:06.255549Z",
     "iopub.status.busy": "2020-10-07T01:28:06.254720Z",
     "iopub.status.idle": "2020-10-07T01:28:19.954669Z",
     "shell.execute_reply": "2020-10-07T01:28:19.955118Z"
    },
    "id": "T7PbRLV74RrU"
   },
   "outputs": [],
   "source": [
    "img = run_deep_dream_with_octaves(img=original_img, step_size=0.01)\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "img = tf.image.resize(img, base_shape)\n",
    "img = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Og0-qLwNbGg"
   },
   "source": [
    "Much better! Play around with the number of octaves, octave scale, and activated layers to change how your DeepDream-ed image looks.\n",
    "\n",
    "Readers might also be interested in [TensorFlow Lucid](https://github.com/tensorflow/lucid) which expands on ideas introduced in this tutorial to visualize and interpret neural networks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deepdream.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
