{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Functions import *\n",
    "import pyautogui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "87965c894d3b7f3b3dfc66d8c2a60efcc08a370d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emotion</td>\n",
       "      <td>pixels</td>\n",
       "      <td>Usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>85 84 90 121 101 102 133 153 153 169 177 189 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     usage\n",
       "0  emotion                                             pixels     Usage\n",
       "1        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "2        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "3        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "4        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "5        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
       "6        2  55 55 55 55 55 54 60 68 54 85 151 163 170 179 ...  Training\n",
       "7        4  20 17 19 21 25 38 42 42 46 54 56 62 63 66 82 1...  Training\n",
       "8        3  77 78 79 79 78 75 60 55 47 48 58 73 77 79 57 5...  Training\n",
       "9        3  85 84 90 121 101 102 133 153 153 169 177 189 1...  Training"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "filname = 'fer2013.csv'\n",
    "label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "names=['emotion','pixels','usage']\n",
    "df=pd.read_csv('fer2013.csv',names=names)\n",
    "im=df['pixels']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "435d0e06553e3de3fd982e4a4a86c28018ac3913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "X, Y = getData(filname)\n",
    "num_class = len(set(Y))\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f3c6bfb7aaf3c25ba7cdd5621e4d62b9eaa5502e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "N, D = X.shape\n",
    "X = X.reshape(N, 48, 48, 1)\n",
    "cascade = load_cascade_classifier_xml()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "be4faef86c3c5635697f10939547edd5c8760308"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "y_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)\n",
    "y_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5004be413385dbdf6c3967d34c59e541095ea667"
   },
   "outputs": [],
   "source": [
    "path_model='model_filter.h5' # save model at this location after each epoch\n",
    "model=my_model() # create the model\n",
    "model.load_weights(path_model)\n",
    "# fit the model\n",
    "# h=model.fit(x=X_train,     \n",
    "#             y=y_train, \n",
    "#             batch_size=64, \n",
    "#             epochs=20, \n",
    "#             verbose=1, \n",
    "#             validation_data=(X_test,y_test),\n",
    "#             shuffle=True,\n",
    "#             callbacks=[\n",
    "#                 ModelCheckpoint(filepath=path_model),\n",
    "#             ]\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "0c7e2abb2a89f4df0d28de0a49db1f60c84fbcf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "objects = ('Republican', 'Republican', 'Republican', 'Republican', 'Republican', 'Republican', 'Republican')\n",
    "y_pos = np.arange(len(objects))\n",
    "print(y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "241291244491cc1e84a45ecb0da66c1c09a4cd7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3589, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32298, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-2rx9f0ng/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-cd4b853bbda9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#logger.debug('postprocess+')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaceframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_faces_in_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcascade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/facialexpression/Official/Emoition-classification-from-facial-expression/Functions.py\u001b[0m in \u001b[0;36mfind_faces_in_img\u001b[0;34m(frame, faceCascade)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_faces_in_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfaceCascade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mframeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     faces = faceCascade.detectMultiScale(\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-2rx9f0ng/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "sfps = 0\n",
    "  \n",
    "fps = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    baseframe = frame\n",
    "    \n",
    "\n",
    "    #logger.debug('postprocess+')\n",
    "    \n",
    "    coords,faces,faceframe = find_faces_in_img(frame,cascade)\n",
    "    if len(faces) < 1:\n",
    "        fps=time()\n",
    "        FPS=1/(fps-sfps)\n",
    "        sfps = fps \n",
    "\n",
    "        FPS = 'FPS:'+str(round(FPS,ndigits=3))\n",
    "        maketextaboveface(baseframe,FPS,(10,70))\n",
    "        cv2.imshow('feed', baseframe)\n",
    "        \n",
    "        c = cv2.waitKey(1)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "        continue\n",
    "    for i in range(0,len(faces)):\n",
    "        face = faces[i]\n",
    "        frame = prep_image(face)\n",
    "\n",
    "        prediction = predict_emotion(frame,model)\n",
    "\n",
    "        text = 'Prediction = '+str(prediction)\n",
    "        maketextaboveface(baseframe,text,(coords[i][2],coords[i][0]))\n",
    "    humans = e.inference(baseframe,\n",
    "                         resize_to_default=(w > 0 and h > 0),\n",
    "                         upsample_size=resize_out_ratio)\n",
    "    image = TfPoseEstimator.draw_humans(baseframe, humans, imgcopy=False)\n",
    "    fps=time()\n",
    "    FPS=1/(fps-sfps)\n",
    "    sfps = fps\n",
    "    FPS = 'FPS:'+str(round(FPS,ndigits=3))+' & We Found '+str(len(faces))+ ' Face(s)!'\n",
    "    maketextaboveface(baseframe,FPS,(10,70))\n",
    "    cv2.imshow('feed', baseframe)\n",
    "    \n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-05 14:44:22,059] [TfPoseEstimator] [INFO] loading graph from /Users/amirmedris/Desktop/facialexpression/tf-pose-estimation/models/graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n",
      "2020-11-05 14:44:22,059 INFO loading graph from /Users/amirmedris/Desktop/facialexpression/tf-pose-estimation/models/graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfPoseEstimator/MobilenetV1/Conv2d_0/weights\n",
      "TfPoseEstimator/image\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/Relu\n",
      "TfPoseEstimator/Conv2d_3_pool\n",
      "TfPoseEstimator/feat_concat/axis\n",
      "TfPoseEstimator/feat_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/concat_stage7/axis\n",
      "TfPoseEstimator/Openpose/concat_stage7\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from tf_pose import common\n",
    "from tf_pose.estimator import TfPoseEstimator\n",
    "from tf_pose.networks import get_graph_path, model_wh\n",
    "\n",
    "logger = logging.getLogger('TfPoseEstimator-WebCam')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "camera = 0\n",
    "resize = '432x368'     # resize images before they are processed\n",
    "resize_out_ratio = 4.0 # resize heatmaps before they are post-processed\n",
    "model = 'mobilenet_thin'\n",
    "show_process = False\n",
    "tensorrt = False       # for tensorrt process\n",
    "# logger.debug('initialization %s : %s' % (args.model, get_graph_path(args.model)))\n",
    "w, h = model_wh(resize)\n",
    "if w > 0 and h > 0:\n",
    "    e = TfPoseEstimator(get_graph_path(model), target_size=(w, h), trt_bool=False)\n",
    "else:\n",
    "    e = TfPoseEstimator(get_graph_path(model), target_size=(432, 368), trt_bool=False)\n",
    "# humans = e.inference(image,\n",
    "#                          resize_to_default=(w > 0 and h > 0),\n",
    "#                          upsample_size=resize_out_ratio)\n",
    "\n",
    "#     #logger.debug('postprocess+')\n",
    "#     image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sfps = 0\n",
    "  \n",
    "fps = 0\n",
    "while True:\n",
    "    img = pyautogui.screenshot()\n",
    "    frame = np.array(img)[1500:,900:]\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    baseframe = frame\n",
    "\n",
    "#     humans = e.inference(baseframe,\n",
    "#                           resize_to_default=(w > 0 and h > 0),\n",
    "#                           upsample_size=resize_out_ratio)\n",
    "\n",
    "#     image = TfPoseEstimator.draw_humans(baseframe, humans, imgcopy=False)\n",
    "    coords,faces,faceframe = find_faces_in_img(frame,cascade)\n",
    "    if len(faces) < 1:\n",
    "        fps=time()\n",
    "        FPS=1/(fps-sfps)\n",
    "        sfps = fps \n",
    "\n",
    "        FPS = 'FPS:'+str(round(FPS,ndigits=3))\n",
    "        maketextaboveface(baseframe,FPS,(10,70))\n",
    "        cv2.imshow('feed', baseframe)\n",
    "        \n",
    "        c = cv2.waitKey(1)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "        continue\n",
    "    for i in range(0,len(faces)):\n",
    "        face = faces[i]\n",
    "        frame = prep_image(face)\n",
    "\n",
    "        prediction = predict_emotion(frame,model)\n",
    "\n",
    "        text = 'Prediction = '+str(prediction)\n",
    "        maketextaboveface(baseframe,text,(coords[i][2],coords[i][0]))\n",
    "    fps=time()\n",
    "    FPS=1/(fps-sfps)\n",
    "    sfps = fps\n",
    "    FPS = 'FPS:'+str(round(FPS,ndigits=3))+' & We Found '+str(len(faces))+ ' Face(s)!'\n",
    "    maketextaboveface(baseframe,FPS,(10,70))\n",
    "    cv2.imshow('feed', baseframe)\n",
    "    \n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Point(x=1246, y=954)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyautogui.position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "sfps = 0\n",
    "  \n",
    "fps = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    baseframe = frame\n",
    "#     humans = e.inference(baseframe,\n",
    "#                          resize_to_default=(w > 0 and h > 0),\n",
    "#                          upsample_size=resize_out_ratio)\n",
    "\n",
    "    #logger.debug('postprocess+')\n",
    "#     image = TfPoseEstimator.draw_humans(baseframe, humans, imgcopy=False)\n",
    "    coords,faces,faceframe = find_faces_in_img(frame,cascade)\n",
    "    \n",
    "        \n",
    "        \n",
    "    for i in range(0,len(faces)):\n",
    "        face = faces[i]\n",
    "        frame = prep_image(face)\n",
    "\n",
    "        prediction = predict_emotion(frame,model)\n",
    "\n",
    "        text = 'Prediction = '+str(prediction)\n",
    "        maketextaboveface(baseframe,text,(coords[i][1],coords[i][0]))\n",
    "    fps=time()\n",
    "    FPS=1/(fps-sfps)\n",
    "    sfps = fps\n",
    "    FPS = 'FPS:'+str(round(FPS,ndigits=3))+' & We Found '+str(len(faces))+ ' Face(s)!'\n",
    "    maketextaboveface(baseframe,FPS,(10,70))\n",
    "    cv2.imshow('feed', baseframe)\n",
    "    \n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess(img):\n",
    "    return tf.cast(img, tf.float32)\n",
    "\n",
    "# Display an image\n",
    "def show(img):\n",
    "    display.display(PIL.Image.fromarray(np.array(tf.cast(img, tf.uint8))))\n",
    "# Create the feature extraction model\n",
    "    \n",
    "def calc_loss(img, model):\n",
    "  # Pass forward the image through the model to retrieve the activations.\n",
    "  # Converts the image into a batch of size 1.\n",
    "    img_batch = tf.expand_dims(img, axis=0)\n",
    "    layer_activations = model(img_batch)\n",
    "    if len(layer_activations) == 1:\n",
    "        layer_activations = [layer_activations]\n",
    "\n",
    "    losses = []\n",
    "    for act in layer_activations:\n",
    "        loss = tf.math.reduce_mean(act)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return  tf.reduce_sum(losses)\n",
    "class DeepDream(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=(\n",
    "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float32),))\n",
    "    def __call__(self, img, steps, step_size):\n",
    "        print(\"Tracing\")\n",
    "        loss = tf.constant(0.0)\n",
    "        for n in tf.range(steps):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # This needs gradients relative to `img`\n",
    "          # `GradientTape` only watches `tf.Variable`s by default\n",
    "                tape.watch(img)\n",
    "                loss = calc_loss(img, self.model)\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
    "                gradients = tape.gradient(loss, img)\n",
    "\n",
    "        # Normalize the gradients.\n",
    "                gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "        \n",
    "        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
    "        # You can update the image by directly adding the gradients (because they're the same shape!)\n",
    "                img = img + gradients*step_size\n",
    "#                 img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "        return loss, img\n",
    "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n",
    "      # Convert from uint8 to the range expected by the model.\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    step_size = tf.convert_to_tensor(step_size)\n",
    "    steps_remaining = steps\n",
    "    step = 0\n",
    "    while steps_remaining:\n",
    "        if steps_remaining>100:\n",
    "            run_steps = tf.constant(100)\n",
    "        else:\n",
    "            run_steps = tf.constant(steps_remaining)\n",
    "        steps_remaining -= run_steps\n",
    "        step += run_steps\n",
    "    \n",
    "        loss, img = deepdream(img, run_steps, tf.constant(step_size))\n",
    "    \n",
    "        display.clear_output(wait=True)\n",
    "        show(img)\n",
    "        print (\"Step {}, loss {}\".format(step, loss))\n",
    "\n",
    "\n",
    "        result = img\n",
    "        display.clear_output(wait=True)\n",
    "        show(result)\n",
    "\n",
    "    return result\n",
    "def generatedeepdreamimg(imagebatch,model,imageposinbatch):\n",
    "    base_model = model\n",
    "\n",
    "    names = [name.name for name in  base_model.layers]\n",
    "    layers = [base_model.get_layer(name).output for name in names[:]]\n",
    "\n",
    "# Create the feature extraction model\n",
    "    dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "    original_img = np.array(imagebatch)\n",
    "    original_img = original_img[imageposinbatch]\n",
    "    original_img = ((original_img)).astype(np.float32)\n",
    "    dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "# show(deprocess(original_img))\n",
    "    deepdream = DeepDream(dream_model)\n",
    "    dream_img = run_deep_dream_simple(img=original_img, \n",
    "                                  steps=1000, step_size=0.5)\n",
    "    return dream_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model\n",
    "\n",
    "names = [name.name for name in  base_model.layers]\n",
    "layers = [base_model.get_layer(name).output for name in names[:]]\n",
    "\n",
    "# Create the feature extraction model\n",
    "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "# coords,faces,faceframe = find_faces_in_img(frame,cascade)\n",
    "face = faces[0]\n",
    "frame = prep_image(face)\n",
    "pickedpic= frame\n",
    "\n",
    "# deepdream = DeepDream(dream_model)\n",
    "# dream_img = run_deep_dream_simple(img=pickedpic, \n",
    "#                                   steps=1000, step_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected any non-tensor type, got a tensor instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    328\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_check_not_tensor\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_not_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m   _ = [_check_failed(v) for v in nest.flatten(values)\n\u001b[0m\u001b[1;32m    282\u001b[0m        if isinstance(v, ops.Tensor)]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_not_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m   _ = [_check_failed(v) for v in nest.flatten(values)\n\u001b[0m\u001b[1;32m    282\u001b[0m        if isinstance(v, ops.Tensor)]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_check_failed\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;31m# it is safe to use here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"Const_10:0\", shape=(), dtype=float32)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-921f62f98437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_deep_dream_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickedpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-0030bafa9bf9>\u001b[0m in \u001b[0;36mrun_deep_dream_simple\u001b[0;34m(img, steps, step_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrun_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepdream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[0;32m--> 263\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    264\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 280\u001b[0;31m       tensor_util.make_tensor_proto(\n\u001b[0m\u001b[1;32m    281\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           allow_broadcast=allow_broadcast))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    454\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mmismatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected any non-tensor type, got a tensor instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected any non-tensor type, got a tensor instead."
     ]
    }
   ],
   "source": [
    "run_deep_dream_simple(pickedpic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pickedpic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('ka', pickedpic[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
